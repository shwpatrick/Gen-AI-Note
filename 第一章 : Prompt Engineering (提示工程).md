[程式碼實作參考](https://www.kaggle.com/code/markishere/day-1-prompting)

## 什麼是提示工程？

大型語言模型是一個通用模型，通常不會針對特定目標進行優化，而提示工程的概念就是透過prompt區域給的提示，使得語言模型產生更符合目標需求的產出。

## 如何增加LLM精準度？給LLM一本說明書

給LLM提示(說明書)可以使得LLM生成內容更接近需求，一般會將提示分成以下幾類：

1. zero shot prompting - 只給任務描述
2. few shot prompting - 除了任務描述，多給數個範例
3. thought prompting -  要求LLM列出解題步驟，一步一步解決問題

## LLM預測下一個文字的機制

從LLM生成文字的原理來看，當LLM生成了一個文字後，對它來說，它會有N個文字適合被接在當前文字後面，而每個文字都有一個機率，機率越高代表越適合做為下一個生成的文字。

## 如何調整LLM生成內容？Sampling Techniques

我們可以控制下個字詞的生產特性去影響LLM的輸出。
當永遠選擇機率最高的銜接字詞，生成文字會越有高的機率接近解答。
但同時，因為穩定性越高，生成文字的創意性會有所降低

Sampling Techniques 在此指的就是選擇下一個生成文字的策略，當遇到下一個生成文字的可選機率分布，該如何選擇哪些內容以作為適合產出的備選文字。

可以理解成以下兩種模式：

1. greedy search - 選擇最穩的輸出 但缺乏創造性
2. temperature, top-k & top-p sampling - 創造性跟精準性間尋找平衡

### Temperature Sampling 

- 高溫 - 預測性低，創意性高
- 低溫 - 預測性高，創意性低

### Top-k & Top-p Sampling

- Top - K : 選擇機率最高的K個字詞作為生產選項
- Top - P : 選擇機率在p以上的字詞作為生產選項

當K值越小，代表下一個詞的選定可能越少，預測性越高。當P值越高，符合P機率以上的可選字詞越少，預測性越高。而當兩個Sampling機制同時套用時，只有同時符合兩個篩選器的詞可以作為生成選項，當K=0時，根據文檔描述，則會隨機選取任意字詞。

## 優化模型

為提升模型效能、降低計算負擔、提高回應速度的技術，  
目的是在不顯著犧牲準確度的情況下改善模型的整體表現  

Quantization - 量化：簡化模型的內部計算，降低數值精度以減少運算量，並盡量在不大幅降低模型精準度的前提下達到效能提升。  
Distillation - 知識蒸餾：通過訓練一個「學生模型」去模仿「教師模型」的行為，學生模型因此能保留教師模型的性能但體積更小、效率更高。  
Output Preserving Model - 輸出保持模型（如 Flash Attention）：透過優化模型結構（如高效注意力機制），在加快運算的同時維持與原始模型一致的輸出結果。  
Prefix Caching - 前文緩存：保存已生成的上下文，避免每次生成時重複計算，從而加快處理速度。  
Speculative Decoding - 試探性解碼：使用輔助模型生成初步草稿，再由主模型進行檢查和修正，以提高生成效率並加快模型回應速度。  
